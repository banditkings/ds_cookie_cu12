# ds_cookie_cu12

A Python data science [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/) for a `poetry` and `pyenv` user, which includes:

1. A `src` folder where the main module for shared code should exist in `src/package_name`
2. pre-commit hooks:
   1. `autopep8`
   2. `poetry export` to sync dependencies with a requirements.txt file
3. A local `.python-version` for `pyenv`
4. Folder structure similar to `kedro`
5. Optional dependencies groups for `plotly` 
6. Optional dependency group for `rapids` assuming you have cuda 12 installed and are using an NVIDIA gpu

## Usage

To create a new project folder that follows this cookiecutter template run:  

```
python -m cookiecutter git@github.com:banditkings/ds_cookie_cu12.git
```

or if using HTTPS auth:

```
python -m cookiecutter https://github.com/banditkings/ds_cookie_cu12.git
```

Then you can navigate to the folder inside and install the library with required dependencies using:

```bash
poetry install --with plotly, rapids
```

## Project Structure

```
{{ cookiecutter.repo_name }}                <- Git repo name
├── .gitignore                              <- Hidden file that prevents staging of unnecessary files to `git`
├── README.md                               <- The top-level README for developers using this project
│
├── data                                    <- Store raw data, features, etc - not committed to git
│   ├── 01_raw                              <-- Raw immutable data
│   ├── 02_intermediate                     <-- Typed data
│   ├── 03_primary                          <-- Domain model data
│   ├── 04_feature                          <-- Model features
│   ├── 05_model_input                      <-- Often called 'master tables'
│   ├── 06_models                           <-- Serialised models
│   ├── 07_model_output                     <-- Data generated by model runs
│   └── 08_reporting                        <-- Ad hoc descriptive cuts
│
├── notebooks                               <- Jupyter notebooks. Naming convention is a number (for ordering),
│                                              the creator's initials, and a short `-` delimited description, e.g.
│                                              `1.0-jqp-initial-data-exploration`.
│
├── .pre-commit-config.yaml                 <- git pre-commit config file
│
├── .python-version                         <- Python version number for local `pyenv` environment
│
├── pyproject.toml                          <- Poetry dependency and environment file
│
├── reports                                 <- Generated analysis as HTML, PDF, LaTeX, etc
│   └── figures                             <- Generated graphics and figures to be used in reporting
│   
└── src                                     <- Source code for this project
    ├── tests                               <- All tests for this package
    └── {{ cookiecutter.package_name }}     <- This package
        ├── data                            <- Scripts to download or generate data
        ├── features                        <- Scripts to turn raw data into features for modeling
        ├── models                          <- Scripts to train models and use trained models to make predictions
        └── visualization                   <- Scripts to create exploratory and results oriented visualizations
```

## Git pre-commit hooks

* [pre-commit docs](https://pre-commit.com/)

The project starts off with pre-commit hooks with some autopep8 rules and trailing whitespace checks, followed by some `poetry` lock fixes that can take a long time to resolve dependencies.

## Other project templates

* [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/)
* [Kedro Starters](https://github.com/kedro-org/kedro-starters)
* [Description of data folder structure](https://towardsdatascience.com/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71)
